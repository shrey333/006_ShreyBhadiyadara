{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import twitter_samples\nimport pandas as pd\nimport tensorflow as tf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-18T06:09:13.295088Z","iopub.execute_input":"2021-09-18T06:09:13.295404Z","iopub.status.idle":"2021-09-18T06:09:13.300166Z","shell.execute_reply.started":"2021-09-18T06:09:13.295372Z","shell.execute_reply":"2021-09-18T06:09:13.299224Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"nltk.download('twitter_samples')\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2021-09-18T06:09:13.309435Z","iopub.execute_input":"2021-09-18T06:09:13.310017Z","iopub.status.idle":"2021-09-18T06:09:53.377191Z","shell.execute_reply.started":"2021-09-18T06:09:13.309973Z","shell.execute_reply":"2021-09-18T06:09:53.376198Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"[nltk_data] Error loading twitter_samples: <urlopen error [Errno -3]\n[nltk_data]     Temporary failure in name resolution>\n[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n[nltk_data]     Temporary failure in name resolution>\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}]},{"cell_type":"code","source":"import re\nimport string\nimport numpy as np\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer","metadata":{"execution":{"iopub.status.busy":"2021-09-18T06:09:53.379413Z","iopub.execute_input":"2021-09-18T06:09:53.379746Z","iopub.status.idle":"2021-09-18T06:09:53.385842Z","shell.execute_reply.started":"2021-09-18T06:09:53.379704Z","shell.execute_reply":"2021-09-18T06:09:53.384707Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"\n#process_tweet(): cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\ndef process_tweet(tweet):\n    \"\"\"Process tweet function.\n    Input:\n        tweet: a string containing a tweet\n    Output:\n        tweets_clean: a list of words containing the processed tweet\n\n    \"\"\"\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english')\n\n    # remove stock market tickers like $GE\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    # remove hyperlinks\n    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n    # tokenize tweets\n\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n\n    tweets_clean = []\n    for word in tweet_tokens:\n        # 1 remove stopwords\n        if word in stopwords_english:\n            continue\n        # 2 remove punctuation\n        if word in string.punctuation:\n            continue\n        # 3 stemming word\n        word = stemmer.stem(word)\n        # 4 Add it to tweets_clean\n        tweets_clean.append(word)\n\n    return tweets_clean","metadata":{"execution":{"iopub.status.busy":"2021-09-18T06:09:53.387677Z","iopub.execute_input":"2021-09-18T06:09:53.387996Z","iopub.status.idle":"2021-09-18T06:09:53.400749Z","shell.execute_reply.started":"2021-09-18T06:09:53.387958Z","shell.execute_reply":"2021-09-18T06:09:53.399733Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"\n#build_freqs counts how often a word in the 'corpus' (the entire set of tweets) was associated with\n# a positive label '1'         or\n# a negative label '0',\n\n#then builds the freqs dictionary, where each key is a (word,label) tuple,\n\n#and the value is the count of its frequency within the corpus of tweets.\n\ndef build_freqs(tweets, ys):\n    \"\"\"Build frequencies.\n    Input:\n        tweets: a list of tweets\n        ys: an m x 1 array with the sentiment label of each tweet\n            (either 0 or 1)\n    Output:\n        freqs: a dictionary mapping each (word, sentiment) pair to its\n        frequency\n    \"\"\"\n    # Convert np array to list since zip needs an iterable.\n    # The squeeze is necessary or the list ends up with one element.\n    # Also note that this is just a NOP if ys is already a list.\n    yslist = np.squeeze(ys).tolist()\n\n    # Start with an empty dictionary and populate it by looping over all tweets\n    # and over all processed words in each tweet.\n    freqs = {}\n\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n\n            #############################################################\n            #Update the count of pair if present, set it to 1 otherwise\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n\n    return freqs","metadata":{"execution":{"iopub.status.busy":"2021-09-18T06:09:53.402141Z","iopub.execute_input":"2021-09-18T06:09:53.402996Z","iopub.status.idle":"2021-09-18T06:09:53.416374Z","shell.execute_reply.started":"2021-09-18T06:09:53.402961Z","shell.execute_reply":"2021-09-18T06:09:53.415184Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"\n# select the set of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')","metadata":{"execution":{"iopub.status.busy":"2021-09-18T06:09:53.418765Z","iopub.execute_input":"2021-09-18T06:09:53.419020Z","iopub.status.idle":"2021-09-18T06:09:54.341220Z","shell.execute_reply.started":"2021-09-18T06:09:53.418991Z","shell.execute_reply":"2021-09-18T06:09:54.340325Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"\n# split the data into two pieces, one for training and one for testing\nfrom sklearn.model_selection import train_test_split\n\ntrain_pos, test_pos = train_test_split(all_positive_tweets, test_size=0.2)\ntrain_neg, test_neg = train_test_split(all_negative_tweets, test_size=0.2)\n\ntrain_x = train_pos + train_neg\ntest_x = test_pos + test_neg","metadata":{"execution":{"iopub.status.busy":"2021-09-18T06:09:54.342525Z","iopub.execute_input":"2021-09-18T06:09:54.342774Z","iopub.status.idle":"2021-09-18T06:09:54.355167Z","shell.execute_reply.started":"2021-09-18T06:09:54.342745Z","shell.execute_reply":"2021-09-18T06:09:54.354145Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"\n# combine positive and negative labels\ntrain_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\ntest_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T06:09:54.356613Z","iopub.execute_input":"2021-09-18T06:09:54.356848Z","iopub.status.idle":"2021-09-18T06:09:54.365365Z","shell.execute_reply.started":"2021-09-18T06:09:54.356823Z","shell.execute_reply":"2021-09-18T06:09:54.364457Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"\n# create frequency dictionary\n#############################################################\nfreqs = build_freqs(train_x, train_y)\n\n# check the output\nprint(\"type(freqs) = \" + str(type(freqs)))\nprint(\"len(freqs) = \" + str(len(freqs.keys())))","metadata":{"execution":{"iopub.status.busy":"2021-09-18T06:09:54.366872Z","iopub.execute_input":"2021-09-18T06:09:54.367153Z","iopub.status.idle":"2021-09-18T06:09:58.573130Z","shell.execute_reply.started":"2021-09-18T06:09:54.367125Z","shell.execute_reply":"2021-09-18T06:09:58.572138Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"type(freqs) = <class 'dict'>\nlen(freqs) = 11330\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Example\nprint('This is an example of a positive tweet: \\n', train_x[0])\nprint('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[0]))","metadata":{"execution":{"iopub.status.busy":"2021-09-18T06:09:58.574419Z","iopub.execute_input":"2021-09-18T06:09:58.574676Z","iopub.status.idle":"2021-09-18T06:09:58.580742Z","shell.execute_reply.started":"2021-09-18T06:09:58.574647Z","shell.execute_reply":"2021-09-18T06:09:58.579704Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"This is an example of a positive tweet: \n @MottFree Thanks for playing #JourneyPS4!  :D\n\nThis is an example of the processed version of the tweet: \n ['thank', 'play', 'journeyp', '4', ':D']\n","output_type":"stream"}]},{"cell_type":"code","source":"\ndef extract_features(tweet, freqs):\n    '''\n    Input:\n        tweet: a list of words for one tweet\n        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n    Output:\n        x: a feature vector of dimension (1,3)\n    '''\n    # tokenizes, stems, and removes stopwords\n    #############################################################\n    output = []\n    for word_l in tweet:\n        word_l = process_tweet(word_l)\n\n        # 3 elements in the form of a 1 x 3 vector\n        x = np.zeros((1, 3))\n\n        #bias term is set to 1\n        x[0,0] = 1\n\n        # loop through each word in the list of words\n        for word in word_l:\n\n            # increment the word count for the positive label 1\n            x[0,1] += freqs.get((word, 1.0),0)\n\n            # increment the word count for the negative label 0\n            x[0,2] += freqs.get((word, 0.0),0)\n\n\n        assert(x.shape == (1, 3))\n        output.append(x)\n    return output\n","metadata":{"execution":{"iopub.status.busy":"2021-09-18T06:09:58.582014Z","iopub.execute_input":"2021-09-18T06:09:58.582256Z","iopub.status.idle":"2021-09-18T06:09:58.595776Z","shell.execute_reply.started":"2021-09-18T06:09:58.582228Z","shell.execute_reply":"2021-09-18T06:09:58.594644Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(2, activation=tf.nn.softmax)\n])","metadata":{"execution":{"iopub.status.busy":"2021-09-18T06:09:58.597269Z","iopub.execute_input":"2021-09-18T06:09:58.597544Z","iopub.status.idle":"2021-09-18T06:09:58.610605Z","shell.execute_reply.started":"2021-09-18T06:09:58.597516Z","shell.execute_reply":"2021-09-18T06:09:58.609547Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer='sgd',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-09-18T06:09:58.612236Z","iopub.execute_input":"2021-09-18T06:09:58.612540Z","iopub.status.idle":"2021-09-18T06:09:58.630890Z","shell.execute_reply.started":"2021-09-18T06:09:58.612510Z","shell.execute_reply":"2021-09-18T06:09:58.629896Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"model.fit(tf.convert_to_tensor(extract_features(train_x, freqs)), train_y, epochs=5)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T06:09:58.631980Z","iopub.execute_input":"2021-09-18T06:09:58.632550Z","iopub.status.idle":"2021-09-18T06:10:04.895270Z","shell.execute_reply.started":"2021-09-18T06:09:58.632506Z","shell.execute_reply":"2021-09-18T06:10:04.894173Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Epoch 1/5\n250/250 [==============================] - 1s 1ms/step - loss: 200.4054 - accuracy: 0.9683\nEpoch 2/5\n250/250 [==============================] - 0s 1ms/step - loss: 63.0609 - accuracy: 0.9952\nEpoch 3/5\n250/250 [==============================] - 0s 1ms/step - loss: 53.3435 - accuracy: 0.9928\nEpoch 4/5\n250/250 [==============================] - 0s 1ms/step - loss: 38.8483 - accuracy: 0.9920\nEpoch 5/5\n250/250 [==============================] - 0s 1ms/step - loss: 15.9421 - accuracy: 0.9931\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7fbd84758990>"},"metadata":{}}]},{"cell_type":"code","source":"model.evaluate(tf.convert_to_tensor(extract_features(test_x, freqs)), test_y)","metadata":{"execution":{"iopub.status.busy":"2021-09-18T06:10:04.899119Z","iopub.execute_input":"2021-09-18T06:10:04.899504Z","iopub.status.idle":"2021-09-18T06:10:06.326881Z","shell.execute_reply.started":"2021-09-18T06:10:04.899461Z","shell.execute_reply":"2021-09-18T06:10:06.325960Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"63/63 [==============================] - 0s 921us/step - loss: 8.0521 - accuracy: 0.9910\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"[8.052050590515137, 0.9909999966621399]"},"metadata":{}}]}]}